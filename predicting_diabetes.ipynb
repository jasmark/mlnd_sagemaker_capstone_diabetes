{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Diabetes\n",
    "\n",
    "### Background: \n",
    "Diabetes is one of the most common and most expensive chronic diseases worldwide. In 2004 it was estimated that in the US alone, approximately 5 million people unknowingly had the disease while another 13 million were aware of their diagnosis. \n",
    "\n",
    "### Problem Statement:  \n",
    "Early detection of the disease can help reduce the risk of serious life changing complications such as premature heart disease, stroke, blindness, limb amputations, and kidney failure.  Models that can help predict an individual with diabetes could be a useful tool to support a physicianâ€™s decision-making process when working with patients. It could also be leveraged to screen populations of patient data to identify patients most likely to have undiagnosed diabetes and intervene with further testing and monitoring. This can be framed as a binary classification problem to separate those who will vs. those who will not develop diabetes.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "> **Citation for the data:** The Pima Indian Diabetes Dataset used originally came from this paper:\n",
    "** Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press. Now available for download via Kaggle [here](https://www.kaggle.com/uciml/pima-indians-diabetes-database).\n",
    "\n",
    "* Note that because the dataset is hosted on Kaggle, it can be downloaded by generating an API token for your user id and installing the Kaggle-cli in your notebook environment. \n",
    "* For simplicity in this notebook, I downloaded and extracted the data on my local machine and then uploaded it into my notebook environment. The file 'diabetes.csv' is the unmodified extracted download file from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ec2-user ec2-user 23873 Aug  5 06:00 data/raw/diabetes.csv\r\n"
     ]
    }
   ],
   "source": [
    "# confirm that the file is accessible\n",
    "!ls -al data/raw/diabetes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the data into a dataframe\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "source_data_dir = 'data/raw'\n",
    "clean_data_dir = 'data/train-test'\n",
    "\n",
    "diabetes_df = pd.read_csv(os.path.join(source_data_dir, 'diabetes.csv'))\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of missing variables:\n",
    "\n",
    "There are missing variable in the dataset although they're not all immediately apparent because they are coded as zeros rather than NaN or NA values. Because zero can be a valid measurement for some of the variables, we'll need to consider them one by one:\n",
    "\n",
    "**Pregnancies** - 0 can be a valid measurement\n",
    "\n",
    "**Glucose:** - 0 is unlikely to be a valid measurement\n",
    "\n",
    "**Blood PRessure:** - 0 is unlikely to be a valid measurement\n",
    "\n",
    "**Skin Thickness:** - 0 is unlikely to be a valid measurement\n",
    "\n",
    "**Insulin:** - 0 is unlikely to be a valid measurement\n",
    "\n",
    "**BMI:** - 0 is unlikely to be a valid measurement\n",
    "\n",
    "**DiabetesPedigreeFunction:** - 0 can be a valid measurement score representing hereditary risk of diabetes based on familial and closeness of genetic relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of zero as value:\n",
      "\t Glucose: 5\n",
      "\t Blood Pressure: 35\n",
      "\t SkinThickness: 227\n",
      "\t Insulin: 374\n",
      "\t BMI: 11\n",
      "\t Glucose + BloodPressure + BMI all 0: 0\n"
     ]
    }
   ],
   "source": [
    "# look at missing values -- indicated with a zero\n",
    "\n",
    "print(\"Counts of zero as value:\")\n",
    "print(\"\\t Glucose: {}\".format(sum(diabetes_df.Glucose == 0)))\n",
    "print(\"\\t Blood Pressure: {}\".format(sum(diabetes_df.BloodPressure == 0)))\n",
    "print(\"\\t SkinThickness: {}\".format(sum(diabetes_df.SkinThickness == 0)))\n",
    "print(\"\\t Insulin: {}\".format(sum(diabetes_df.Insulin == 0)))\n",
    "print(\"\\t BMI: {}\".format(sum(diabetes_df.BMI == 0)))\n",
    "\n",
    "print(\"\\t Glucose + BloodPressure + BMI all 0: {}\".format(sum((diabetes_df.Glucose == 0) &\n",
    "                                                          (diabetes_df.BloodPressure == 0) &\n",
    "                                                          (diabetes_df.BMI == 0))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My intention is to use tree based models to because they can offer more straightforward explainability than non-linear models and in a healthcare context that can be important to those using the output of the model. Linear models such as trees don't necesarily need to have normalized data values. Likewise they should be able to make cuts around the missing values indicated as zeros. Based on these factors, in my initial modeling, I'm not going to normalize the data or remove the missing values. Depending on model performance this is something I will revisit if needed, however in a real world situation, where some of these data elements will likely be missing at prediction time, having a model that is robust enough to make good predictions even in their absence would have a lot of value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (614, 8) Training label shape: (614,)\n",
      "Validation data shape: (77, 8) Validation label shape: (77,)\n",
      "Testing data shape: (77, 8) Testing label shape: (77,)\n"
     ]
    }
   ],
   "source": [
    "# preprocess data - wrap in a function in case add'l preprocessing steps are needed\n",
    "# to begin with, just splitting to train/test sets\n",
    "# hold out 1/4 of data for testing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "# rearrange to pass to train_test_split\n",
    "labels = diabetes_df.Outcome\n",
    "data = diabetes_df.drop(columns = ['Outcome'])\n",
    "    \n",
    "random_state = 27\n",
    "\n",
    "# split into train, valtest splits, putting 80% of data into training\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(data, labels, \n",
    "                                                          random_state = random_state,\n",
    "                                                          test_size = .2)\n",
    "\n",
    "# split remaining 20% of data in valtest equally so 10% validation / 10% testing\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest,\n",
    "                                                random_state = random_state,\n",
    "                                                test_size = .5)\n",
    "\n",
    "# print sizes to make sure we got what we expected\n",
    "print(\"Training data shape: {} Training label shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"Validation data shape: {} Validation label shape: {}\".format(X_val.shape, y_val.shape))\n",
    "print(\"Testing data shape: {} Testing label shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure data for processing by training model and store as csv\n",
    "def make_csv(x, y, filename, data_dir):\n",
    "    '''Merges features and labels and converts them into one csv file with labels in the first column.\n",
    "       :param x: Data features\n",
    "       :param y: Data labels\n",
    "       :param file_name: Name of csv file, ex. 'train.csv'\n",
    "       :param data_dir: The directory where files will be saved\n",
    "       '''\n",
    "\n",
    "    # make data dir, if it does not exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    \n",
    "    df_X = pd.DataFrame(x)\n",
    "    df_y = pd.DataFrame(y)\n",
    "    \n",
    "    df_all = pd.concat([df_y,df_X], axis=1)\n",
    "    \n",
    "    df_all.to_csv(data_dir + '/' + filename, index = False, header=False)\n",
    "\n",
    "    # nothing is returned, but a print statement indicates that the function has run\n",
    "    print('Path created: '+str(data_dir)+'/'+str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path created: data/train-test/train.csv\n",
      "Path created: data/train-test/validation.csv\n",
      "Path created: data/train-test/test.csv\n"
     ]
    }
   ],
   "source": [
    "make_csv(X_train, y_train, 'train.csv', clean_data_dir)\n",
    "make_csv(X_val, y_val, 'validation.csv', clean_data_dir)\n",
    "make_csv(X_test, y_test, 'test.csv', clean_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-501454055284/capstone\n"
     ]
    }
   ],
   "source": [
    "# copy data to s3\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'capstone'\n",
    "\n",
    "# upload all data to S3\n",
    "input_data = sagemaker_session.upload_data(path=clean_data_dir, bucket=bucket, key_prefix=prefix)\n",
    "print(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build xgBoost estimator\n",
    "# As stated above, we use this utility method to construct the image name for the training container.\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(sagemaker_session.boto_region_name, 'xgboost')\n",
    "\n",
    "# Now that we know which container to use, we can construct the estimator object.\n",
    "xgb = sagemaker.estimator.Estimator(container, # The image name of the training container\n",
    "                                    role,      # The IAM role to use (our current role in this case)\n",
    "                                    train_instance_count=2, # The number of instances to use for training\n",
    "                                    train_instance_type='ml.m4.xlarge', # The type of instance to use for training\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                                                        # Where to save the output (the model artifacts)\n",
    "                                    sagemaker_session=sagemaker_session) # The current SageMaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgb, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'validation:auc', # The metric used to compare trained models.\n",
    "                                               objective_type = 'Maximize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 10, # The total number of models to train\n",
    "                                               max_parallel_jobs = 5, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'max_depth': IntegerParameter(3, 12),\n",
    "                                                    'eta'      : ContinuousParameter(0.05, 0.5),\n",
    "                                                    'min_child_weight': IntegerParameter(2, 8),\n",
    "                                                    'subsample': ContinuousParameter(0.5, 0.9),\n",
    "                                                    'gamma': ContinuousParameter(0, 10),\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# This is a wrapper around the location of our train and validation data, to make sure that SageMaker\n",
    "# knows our data is in csv format.\n",
    "\n",
    "train_file_path = os.path.join(input_data, 'train.csv')\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_file_path, content_type='csv')\n",
    "\n",
    "val_file_path = os.path.join(input_data, 'validation.csv')\n",
    "s3_input_val = sagemaker.s3_input(s3_data=val_file_path, content_type='csv')\n",
    "\n",
    "#xgb.fit({'train': s3_input_train, 'validation': s3_input_val})\n",
    "xgb_hyperparameter_tuner.fit({'train': s3_input_train, 'validation': s3_input_val})\n",
    "xgb_hyperparameter_tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-12 00:24:12 Starting - Preparing the instances for training\n",
      "2019-08-12 00:24:12 Downloading - Downloading input data\n",
      "2019-08-12 00:24:12 Training - Training image download completed. Training in progress.\n",
      "2019-08-12 00:24:12 Uploading - Uploading generated training model\n",
      "2019-08-12 00:24:12 Completed - Training job completed\u001b[32mArguments: train\u001b[0m\n",
      "\u001b[32m[2019-08-12:00:21:50:INFO] Running distributed xgboost training.\u001b[0m\n",
      "\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[2019-08-12:00:21:51:INFO] Running distributed xgboost training.\u001b[0m\n",
      "\u001b[31m[2019-08-12:00:21:55:INFO] Number of hosts: 2, master IP address: 10.0.207.184, host IP address: 10.0.207.184.\u001b[0m\n",
      "\u001b[32m[2019-08-12:00:21:54:INFO] Number of hosts: 2, master IP address: 10.0.207.184, host IP address: 10.0.252.183.\u001b[0m\n",
      "\u001b[32m[2019-08-12:00:21:54:INFO] Finished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[32mstarting datanode, logging to /opt/amazon/hadoop/logs/hadoop--datanode-ip-10-0-252-183.us-west-2.compute.internal.out\u001b[0m\n",
      "\u001b[31m[2019-08-12:00:21:55:INFO] Finished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[31mstarting namenode, logging to /opt/amazon/hadoop/logs/hadoop--namenode-ip-10-0-207-184.us-west-2.compute.internal.out\u001b[0m\n",
      "\u001b[32mstarting nodemanager, logging to /opt/amazon/hadoop/logs/yarn--nodemanager-ip-10-0-252-183.us-west-2.compute.internal.out\u001b[0m\n",
      "\u001b[32m[2019-08-12:00:22:00:INFO] File size need to be processed in the node: 0.01mb. Available memory size in the node: 8323.04mb\u001b[0m\n",
      "\u001b[31mstarting resourcemanager, logging to /opt/amazon/hadoop/logs/yarn--resourcemanager-ip-10-0-207-184.us-west-2.compute.internal.out\u001b[0m\n",
      "\u001b[31mstarting datanode, logging to /opt/amazon/hadoop/logs/hadoop--datanode-ip-10-0-207-184.us-west-2.compute.internal.out\u001b[0m\n",
      "\u001b[31mstarting nodemanager, logging to /opt/amazon/hadoop/logs/yarn--nodemanager-ip-10-0-207-184.us-west-2.compute.internal.out\u001b[0m\n",
      "\u001b[31m[2019-08-12:00:22:07:INFO] File size need to be processed in the node: 0.01mb. Available memory size in the node: 7902.86mb\u001b[0m\n",
      "\u001b[31m[2019-08-12:00:22:07:INFO] HTTP server started....\u001b[0m\n",
      "\u001b[31m[2019-08-12:00:22:07:INFO] Memory/core ratio is 7.83\u001b[0m\n",
      "\u001b[31m[2019-08-12:00:22:07:INFO] Yarn setup: number of workers: 2, physical cores per worker: 2, physical memory per worker: 15.67g.\u001b[0m\n",
      "\u001b[31m[2019-08-12:00:22:07:INFO] Yarn job submitted successfully.\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:07,386 INFO start listen on 10.0.207.184:9091\u001b[0m\n",
      "\u001b[31m/xgboost/dmlc-core/tracker/dmlc_tracker/yarn.py:37: UserWarning: cannot find \"/xgboost/dmlc-core/tracker/dmlc_tracker/../yarn/dmlc-yarn.jar\", I will try to run build\n",
      "  warnings.warn(\"cannot find \\\"%s\\\", I will try to run build\" % YARN_JAR_PATH)\u001b[0m\n",
      "\u001b[31msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:37: warning: Signal is internal proprietary API and may be removed in a future release\u001b[0m\n",
      "\u001b[31mimport sun.misc.Signal;\n",
      "               ^\u001b[0m\n",
      "\u001b[31msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:38: warning: SignalHandler is internal proprietary API and may be removed in a future release\u001b[0m\n",
      "\u001b[31mimport sun.misc.SignalHandler;\n",
      "               ^\u001b[0m\n",
      "\u001b[31msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:276: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        Signal intSignal = new Signal(\"INT\");\n",
      "        ^\u001b[0m\n",
      "\u001b[31msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:276: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        Signal intSignal = new Signal(\"INT\");\n",
      "                               ^\u001b[0m\n",
      "\u001b[31msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:277: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        Signal.handle(intSignal, handler);\n",
      "        ^\u001b[0m\n",
      "\u001b[31msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:332: warning: SignalHandler is internal proprietary API and may be removed in a future release\n",
      "    class CtrlCHandler implements SignalHandler{\n",
      "                                  ^\u001b[0m\n",
      "\u001b[31msrc/main/java/org/apache/hadoop/yarn/dmlc/Client.java:339: warning: Signal is internal proprietary API and may be removed in a future release\n",
      "        public void handle(Signal signal){\n",
      "                           ^\u001b[0m\n",
      "\u001b[31mNote: src/main/java/org/apache/hadoop/yarn/dmlc/ApplicationMaster.java uses unchecked or unsafe operations.\u001b[0m\n",
      "\u001b[31mNote: Recompile with -Xlint:unchecked for details.\u001b[0m\n",
      "\u001b[31m7 warnings\u001b[0m\n",
      "\u001b[31m19/08/12 00:22:11 INFO client.RMProxy: Connecting to ResourceManager at algo-1/10.0.207.184:8032\u001b[0m\n",
      "\u001b[31m19/08/12 00:22:12 INFO dmlc.Client: HDFS temp directory do not exist, creating.. /tmp\u001b[0m\n",
      "\u001b[31m19/08/12 00:22:13 INFO dmlc.Client: jobname=DMLC[nworker=2]:python,username=root\u001b[0m\n",
      "\u001b[31m19/08/12 00:22:13 INFO dmlc.Client: Submitting application application_1565569322673_0001\u001b[0m\n",
      "\u001b[31m19/08/12 00:22:13 INFO impl.YarnClientImpl: Submitted application application_1565569322673_0001\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:20,911 INFO @tracker All of 2 nodes getting started\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:21,121 INFO [0]#011train-auc:0.791565#011validation-auc:0.757628\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:21,122 INFO Multiple eval metrics have been passed: 'validation-auc' will be used for early stopping.\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:21,122 INFO Multiple eval metrics have been passed: 'validation-auc' will be used for early stopping.\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:21,122 INFO Will train until validation-auc hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:21,300 INFO [1]#011train-auc:0.823143#011validation-auc:0.75909\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:21,476 INFO [2]#011train-auc:0.8334#011validation-auc:0.752104\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:21,652 INFO [3]#011train-auc:0.849632#011validation-auc:0.81376\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:21,828 INFO [4]#011train-auc:0.859029#011validation-auc:0.827805\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:22,004 INFO [5]#011train-auc:0.864669#011validation-auc:0.847831\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:22,180 INFO [6]#011train-auc:0.866427#011validation-auc:0.841392\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:22,360 INFO [7]#011train-auc:0.872184#011validation-auc:0.848185\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:22,536 INFO [8]#011train-auc:0.87695#011validation-auc:0.837375\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:22,712 INFO [9]#011train-auc:0.877801#011validation-auc:0.846177\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:22,888 INFO [10]#011train-auc:0.879837#011validation-auc:0.845527\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:23,064 INFO [11]#011train-auc:0.878237#011validation-auc:0.848894\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:23,240 INFO [12]#011train-auc:0.88295#011validation-auc:0.846177\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:23,420 INFO [13]#011train-auc:0.884153#011validation-auc:0.84281\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:23,596 INFO [14]#011train-auc:0.884153#011validation-auc:0.84281\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:23,772 INFO [15]#011train-auc:0.884153#011validation-auc:0.84281\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:23,948 INFO [16]#011train-auc:0.884408#011validation-auc:0.846177\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:24,124 INFO [17]#011train-auc:0.884408#011validation-auc:0.846177\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:24,300 INFO [18]#011train-auc:0.885801#011validation-auc:0.84786\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:24,480 INFO [19]#011train-auc:0.88534#011validation-auc:0.849249\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:24,656 INFO [20]#011train-auc:0.88534#011validation-auc:0.849249\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:24,832 INFO [21]#011train-auc:0.88534#011validation-auc:0.849249\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:25,010 INFO [22]#011train-auc:0.886885#011validation-auc:0.847565\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:25,188 INFO [23]#011train-auc:0.886885#011validation-auc:0.847565\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:25,364 INFO [24]#011train-auc:0.888185#011validation-auc:0.83643\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:25,540 INFO [25]#011train-auc:0.888634#011validation-auc:0.836755\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:25,717 INFO [26]#011train-auc:0.888634#011validation-auc:0.836755\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:25,892 INFO [27]#011train-auc:0.888847#011validation-auc:0.843874\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:26,077 INFO [28]#011train-auc:0.888847#011validation-auc:0.843874\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:26,252 INFO [29]#011train-auc:0.888847#011validation-auc:0.843874\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:26,253 INFO Stopping. Best iteration:\u001b[0m\n",
      "\u001b[31m[19]#011train-auc:0.88534#011validation-auc:0.849249\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:26,254 INFO Finished training\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:26,255 INFO Finished training\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:26,256 INFO @tracker All nodes finishes job\u001b[0m\n",
      "\u001b[31m2019-08-12 00:22:26,256 INFO @tracker 5.34411001205 secs between node start and job finish\u001b[0m\n",
      "\u001b[32m[2019-08-12:00:24:02:INFO] Master host is not alive. Training might have finished. Shutting down.... Check the logs for algo-1 machine.\u001b[0m\n",
      "Billable seconds: 338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------!CPU times: user 614 ms, sys: 33 ms, total: 647 ms\n",
      "Wall time: 8min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "xgb_best_model = sagemaker.estimator.Estimator.attach(xgb_hyperparameter_tuner.best_training_job())\n",
    "# deploy model for testing\n",
    "predictor = xgb_best_model.deploy(initial_instance_count=1,\n",
    "                                  instance_type = 'ml.t2.medium')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tuning_objective_metric': 'validation:auc',\n",
       " 'early_stopping_rounds': '10',\n",
       " 'eta': '0.2456946643830148',\n",
       " 'gamma': '3.791865101543357',\n",
       " 'max_depth': '11',\n",
       " 'min_child_weight': '8',\n",
       " 'num_round': '500',\n",
       " 'objective': 'binary:logistic',\n",
       " 'subsample': '0.5196426682128708'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_best_model.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct number of results returned.\n"
     ]
    }
   ],
   "source": [
    "# We need to tell the endpoint what format the data we are sending is in\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import numpy as np\n",
    "predictor.content_type = 'text/csv'\n",
    "predictor.serializer = csv_serializer\n",
    "\n",
    "y_preds = predictor.predict(X_test.values).decode('utf-8')\n",
    "\n",
    "# predictions is currently a comma delimited string and so we would like to break it up\n",
    "# as a numpy array.\n",
    "y_preds = np.fromstring(y_preds, sep=',')\n",
    "\n",
    "# make sure the right number of labels are returned\n",
    "assert len(y_preds)==len(y_test), 'Unexpected number of predictions.'\n",
    "print('Correct number of results returned.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.85      0.84        55\n",
      "           1       0.60      0.55      0.57        22\n",
      "\n",
      "   micro avg       0.77      0.77      0.77        77\n",
      "   macro avg       0.71      0.70      0.71        77\n",
      "weighted avg       0.76      0.77      0.76        77\n",
      "\n",
      "ROC_AUC_Score: 0.8438016528925619\n",
      "Accuracy Score: 0.7662337662337663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_preds_ints = [round(num) for num in y_preds.squeeze()]\n",
    "\n",
    "print(classification_report(y_test, y_preds_ints, labels = [0,1]))\n",
    "print(\"ROC_AUC_Score: {}\".format(roc_auc_score(y_test, y_preds)))\n",
    "print(\"Accuracy Score: {}\".format(accuracy_score(y_test, y_preds_ints)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up and summarize\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
